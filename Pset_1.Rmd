---
title: 'Problem Set #1'
subtitle: 'API 222A: Machine Learning and Big Data Analytics'
author: "Cian Stryker"
date: "9/15/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# Loading all packages and setting working directory. 

library(tidyverse)
library(class)
library(AER)
library(FNN)

setwd("C:/Users/cians/Desktop/Machine Learning/Machine_Learning_Pset_1")
```

# Conceptual Questions


#### Question 1

\hfill\break


a.) This is a classification question and we are interested in prediction.

b.) This is a regression question and we are interested in prediction. 

c.) This is a regression question and we are interested in inference.  


#### Question 2

\hfill\break


a.) False

b.) False

c.) True 

d.) False


#### Question 3

Models that use few parameters typically have high bias and low variance, but models with many parameters typically have low bias and high variance. A good model typically finds a balance between bias and variance in order to minimize total error. 

<br> 


# Data Questions

<br> 

```{r Setup }

# Loading in the data and renaming. 

data(CASchools)

data <- CASchools

```

```{r Question 1}

# Check number of observations. 

q1 <- nrow(data)
```

1.) There are `r q1` observations in the data set. 

<br>

```{r Question 2}

# Check variables. 

q2 <- ncol(data)
```

2.) There are `r q2` variables in the data set.

<br>

```{r Question 3}

# Quick check to see if anything is categorical. 

 q3 <- sapply(data, class)
```

3.) None of the columns are categorical. 

<br>

```{r Question 4}

# Just dropped the NAs and saw that the number of 
# observations did not change. 

data_na_check <- data %>%
  drop_na()

```

4.) There are no missing values. 

<br>

```{r Question 5}

# Created a subset to grab just students and cacluate
# the mean. Then I formatted it to two decimal points. 

q5_1 <- data %>%
  select(students) %>%
  summarize(x = mean(students))

q5 <- format(round(q5_1, 2), nsmall = 2)
```

5.) The mean number of students is `r q5`. 

<br>

```{r Question 6}

# Subsetted for computer, cacculated standard deviation
# and then formatted. 

q6_1 <- data %>%
  select(computer) %>%
  summarize(x = sd(computer))

q6 <- format(round(q6_1, 2), nsmall = 2)

```

6.) The standard deviation of the number of computers is `r q6`. 

<br>

7.) Calworks means the percent of stunts who qualify for Calworks or income assistance. 

<br>

```{r Question 8}

# Subsetted and removed all observations below 500 students
# then I took the number of observations and subtracted the  
# it from the total in q2.  

q8_1 <- data %>%
  filter(students > 500) %>%
  nrow()

q8 <- q1 - q8_1

```

8.) `r q8` observations would be dropped if you limit the sample to schools with 500+ students. 

<br>

```{r Data Sorting}

# Cleaning up the data and splitting it between test and training
# data following the instructions of the pset. 

test <- data %>%
  arrange(students) %>%
  head(80) %>%
  select(students, teachers, calworks, lunch, computer, expenditure, income, english, read)

training <- data %>%
  arrange(desc(students)) %>%
  head(200) %>%
  select(students, teachers, calworks, lunch, computer, expenditure, income, english, read)

```

9.) An issue with splitting the data this way is that your training data is using only large schools while your test data is largely using small schools. Smaller schools and larger schools tend to differ from each other within a few categories. For example, small schools within urban centers are often private schools with radically different socio-economic levels and demographics than larger urban schools. Small schools could also be rural schools that also differ strongly from the typical large school that is urban. Splitting the data based on student size and making the test use small student bodies while the training data uses large student bodies may negatively impact how well an algorithm will be able to translate to the test data because of these differences. 

<br>

```{r Question 10}

# Ran a standard linear regression per the instructions and then
# ran that result on the test data. 

model_1 <- lm(data = training, read ~ students + teachers + calworks + lunch + computer + expenditure + income + english)

prediction_model_1 <- predict(model_1, test[,-9])

MSE_model_1 <- mean((prediction_model_1 - test[,9])^2)

q10 <- format(round(MSE_model_1, 2), nsmall = 2)

```

10.) The Mean Squared Error for the test data using the linear model from the training data is `r q10`. 

\hfill\break


11.) 

```{r Question 11}

# Ran another linear regression, grabbed the coefficients and 
# then formatted the results. 

model_3 <- lm(data = training, read ~ students + teachers + computer)

q11_1 <- model_3$coefficients

q11 <- format(round(q11_1, 2), nsmall = 2)

q11

```

The coefficient on computers is 0.02.

\hfill\break


12.) 

```{r Question 12}

# Yet another linear regression with formatting. 

model_4 <- lm(data = training, read ~ students + teachers + income + computer)

q12_1 <- model_4$coefficients

q12 <- format(round(q12_1, 2), nsmall = 2)

q12
```


a.) The coefficient on computers is now 0.01. 

b.) This implies that reading scores has a positive correlation with income and that computer scores and income are closely correlated to each other as well. In adding income as a variable to the model, the effect size of computer decreased, which suggests that income captures some of computer's effect. 

\hfill\break


```{r Question 13}

knn_reg_1 <- knn.reg(training, test, training[,9], k = 5)

mse_knn_1 <- mean((knn_reg_1$pred - test[,9])^2)

q13 <- format(round(mse_knn_1, 2), nsmall = 2)

```

13.) The Mean Squared Error on the test data with k = 5 is `r q13`.


```{r Question 14}

knn_reg_2 <- knn.reg(training, test, training[,9], k = 10)

mse_knn_2 <- mean((knn_reg_2$pred - test[,9])^2)

q14 <- format(round(mse_knn_2, 2), nsmall = 2)

```

14.) The Mean Squared Error on the test data with k = 10 is `r q14`.
